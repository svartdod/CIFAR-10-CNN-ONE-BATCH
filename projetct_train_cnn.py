# -*- coding: utf-8 -*-
"""Projetct_train_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T50ybhGhgNc_l18FVUFvKiy7zMdwb6Id
"""

#Deep Learning LSTM Trainging + TEST PART 
#HARROUZ MOUAD Faculty of Science and Technology MASTER2 ISICG 20208044
#Biyuzan  HAMZA Faculty of Science and Technology Master2 ISICG 20187435

import tensorflow as tf
import numpy as np
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten ,Conv2D,MaxPooling2D,BatchNormalization
import matplotlib.pyplot as plt 
from random import randrange
from tensorflow.keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
import time

#Read File of test & training 
def unpickle(file):
    import pickle
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

def plot_random(features,labels,numbers):
  for i in range (numbers):
    index=randrange(features.shape[0])
    plt.figure(figsize=(20,2))
    plt.imshow(features[index])
    plt.xlabel(classnames[int(labels[index])])

classnames = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

#Load data 
file= unpickle('InputData/data_batch_1')
#file_test=unpickle('test_batch')
#x_test=file_test[b'data']
x_train= file[b'data']
y_train= file[b'labels']
#y_test=file_test[b'labels']
#y_test=np.vstack(y_test)
y_train=np.vstack(y_train)
y_train_temp=y_train

x_train=np.dstack((x_train[:, :1024], x_train[:, 1024:2048], x_train[:, 2048:]))
x_train = x_train.reshape((x_train.shape[0], 32, 32, 3))
#x_test=np.dstack((x_test[:, :1024], x_test[:, 1024:2048], x_test[:, 2048:]))
#x_test = x_test.reshape((x_test.shape[0], 32, 32, 3))
print("============= LOAD DATA =============")
print("We have  : ", y_train.shape," paradigms of size : ",x_train.shape)
print("Display Some exemples  : ")
plot_random(x_train,y_train,5)

#Using One Hot encoding for label's part 

y_train=to_categorical(y_train,10)
#y_test=to_categorical(y_test,10)
print (" === Encoding output Done ===")

from random import randint

class_to_demonstrate = 0

while (sum(y_train_temp == class_to_demonstrate) > 4):
    tmp_idxs_to_use = np.where(y_train_temp == class_to_demonstrate)

    # create new plot window
    plt.figure()

    # plot 4 images as gray scale
    for j in range(4):
      index=randint(0, len(tmp_idxs_to_use[0]))
      plt.subplot(221+j)
      plt.imshow(x_train[tmp_idxs_to_use[0][index]])
      tmp_title = 'Objects  considered as ' + classnames[int(class_to_demonstrate)]
      plt.suptitle(tmp_title)
      plt.savefig("OutputData/"+str(classnames[int(class_to_demonstrate)])+str(time.time())+".png")

    # show the plot
    plt.show()
    
    plt.pause(2)

    # update the class to demonstrate index
    class_to_demonstrate = class_to_demonstrate + 1

#Normalization of input using min & max 
x_train = x_train.astype('float32')
#x_test=x_test.astype('float32')
# normalization's results gonna be between 0 & 1 
x_train=x_train/255.0
#x_test=x_test/255.0
print (" === Normalization Input  Done ===")

#First model
"""
print(" === Creation of convolution ===")
model = Sequential()
#Convolution part & feature extraction 
model.add(Conv2D(32,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding='same',input_shape=(32,32,3)))
model.add(BatchNormalization())
model.add(Conv2D(32,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(64,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3))

model.add(Conv2D(128,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128,kernel_size=(3,3),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.4))
print(" === Creation done ===")
"""

#second model
"""
print(" === Creation of convolution ===")
model = Sequential()
model.add(Conv2D(96,kernel_size=(5,5),activation='relu',kernel_initializer='he_uniform',padding='same',input_shape=(32,32,3)))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(80,kernel_size=(5,5),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Conv2D(96,kernel_size=(5,5),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(Conv2D(64,kernel_size=(5,5),activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(256,activation='relu',kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(10,activation='softmax'))
print(" === Creation done ===")
"""

#third model
print(" === Creation of convolution ===")
model=Sequential()
model.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same',input_shape=(32,32,3)))
model.add(BatchNormalization())
model.add(Conv2D(32,kernel_size=(3,3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3))


model.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64,kernel_size=(3,3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.5))

model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128,kernel_size=(3,3),activation='relu',padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.5))
print(" === Creation of done ===")

#NN Part
print(" === Creation of model ===")
model.add(Flatten())
model.add(Dense(128,activation='relu',kernel_initializer='he_uniform'))
model.add(Dropout(0.5))
model.add(Dense(10,activation='softmax'))
print(" === Creation of Done ===")
print(" === Architecture of model ===")
model.summary()

#Hyper parametre & compiling model 
'''
model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False, name='adam',),
              loss='categorical_crossentropy',
              metrics=['accuracy'])
'''

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#fit model 
print("=== Learning Phase ===")
#delete comment to train without data augmentation 
#history=model.fit(x_train,y_train,epochs=100,batch_size=32,validation_split=0.2, verbose=1)
print("=== Learning Done ===")

#DATA AUGEMENTATION TO PROVIDE BETTER RESULTS
x_val=x_train[:1000]
y_val=y_train[:1000]

x_train=x_train[1000:]
y_train=y_train[1000:]
y_train_temp=y_train

# training using Data augementation
dataAugmentation= ImageDataGenerator(width_shift_range=0.1,height_shift_range=0.1,horizontal_flip=True,shear_range=0.2,
    zoom_range=0.2)

generatordata=dataAugmentation.flow(x_train,y_train,batch_size=32)
history=model.fit_generator(generatordata,steps_per_epoch=x_train.shape[0]//32,epochs=200,validation_data=(x_val,y_val), verbose=1)

#Draw Results 
#accurcy 
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
plt.savefig("OutputData/AccurcyGraph.png")
#loss 
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
plt.savefig("OutputData/LossGraph.png")

#Saving model 
print("Saving Model ...")
model_name = 'OutputData/CIFARE'+str(time.time())+'.h5'
model.save(model_name)
print ("=== Model saved === ")

#score = model.evaluate(x_test, y_test, verbose=1)
#print(score)

"""test_preds = model.predict(x_test)
pred_classes = np.argmax(test_preds, axis=1)
"

fig, axes = plt.subplots(5, 5, figsize=(15,15))
axes = axes.ravel()
real_class=np.argmax(y_test, axis=1)

for i in range( 25):
    axes[i].imshow(x_test[i])
    axes[i].set_title("True: %s \nPredict: %s" % (pred_classes[i], real_class[i]))  
    axes[i].axis('off')
    plt.subplots_adjust(wspace=1)
"""

#all metrics 
"""
from sklearn.metrics import classification_report
print(classification_report(real_class,pred_classes))
"""



#Confusion Matrix 
"""
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
Matrix_confusion = confusion_matrix(real_class,pred_classes)
plot=ConfusionMatrixDisplay(Matrix_confusion,display_labels=classnames)
plt.figure(figsize=(12,12))
_,axe=plt.subplots(figsize=(12,12))
plot.plot(cmap=plt.cm.Blues,ax=axe)
plt.show()
"""

